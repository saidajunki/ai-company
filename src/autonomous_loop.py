"""Autonomous Loop â€” ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å†…ã§è‡ªå¾‹ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’åˆ¶å¾¡ã™ã‚‹.

tick() ã¯ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‹ã‚‰å‘¼ã°ã‚Œã€WIP/äºˆç®—ãƒã‚§ãƒƒã‚¯å¾Œã«
pendingã‚¿ã‚¹ã‚¯ã‚’1ã¤é¸æŠã—ã¦å®Ÿè¡Œã™ã‚‹ã€‚pendingãŒãªã‘ã‚Œã°LLMã«ææ¡ˆã‚’ä¾é ¼ã™ã‚‹ã€‚

Requirements: 3.2, 3.3, 3.4, 3.5, 3.6, 3.7
"""

from __future__ import annotations

import logging
import os
import re
from typing import TYPE_CHECKING

from llm_client import LLMError
from models import TaskEntry
from response_parser import parse_response
from shell_executor import execute_shell

if TYPE_CHECKING:
    from manager import Manager

logger = logging.getLogger(__name__)

DEFAULT_WIP_LIMIT = 3
MAX_TASK_TURNS = 50


class AutonomousLoop:
    """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å†…ã§è‡ªå¾‹ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’åˆ¶å¾¡ã™ã‚‹."""

    def __init__(self, manager: Manager) -> None:
        self.manager = manager

    def tick(self) -> None:
        """1ã‚µã‚¤ã‚¯ãƒ«åˆ†ã®è‡ªå¾‹å®Ÿè¡Œã‚’è¡Œã†.

        1. WIPã«ç©ºããŒã‚ã‚‹ã‹ç¢ºèª
        2. äºˆç®—ã«ä½™è£•ãŒã‚ã‚‹ã‹ç¢ºèª
        3. pendingã‚¿ã‚¹ã‚¯ã‚’é¸æŠï¼ˆãªã‘ã‚Œã°LLMã«ææ¡ˆã‚’ä¾é ¼ï¼‰
        4. ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        5. çµæœã‚’å ±å‘Š
        """
        try:
            # 1. WIP check
            running = self.manager.task_queue.list_by_status("running")
            wip_limit = self._get_wip_limit()
            if len(running) >= wip_limit:
                logger.info("WIP full (%d/%d), skipping tick", len(running), wip_limit)
                return

            # 2. Budget check
            if self.manager.check_budget():
                logger.info("Budget exceeded, skipping tick")
                return

            # 3. Pick task
            task = self._pick_task()
            if task is None:
                # No pending tasks â€” ask LLM to propose new ones
                proposed = self._propose_tasks()
                if not proposed:
                    logger.info("No tasks proposed, skipping tick")
                    return
                task = self._pick_task()
                if task is None:
                    return

            # 4. Execute task
            self._execute_task(task)

        except Exception:
            logger.exception("Error in autonomous loop tick")

    def _pick_task(self) -> TaskEntry | None:
        """æ¬¡ã«å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’é¸æŠã™ã‚‹."""
        return self.manager.task_queue.next_pending()

    def _propose_tasks(self) -> list[TaskEntry]:
        """LLMã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®ææ¡ˆã‚’ä¾é ¼ã™ã‚‹."""
        if self.manager.llm_client is None:
            logger.warning("LLM client not configured, cannot propose tasks")
            return []

        try:
            vision_text = self.manager.vision_loader.load()
        except Exception:
            logger.warning("Failed to load vision", exc_info=True)
            vision_text = ""

        # Creator score policy (optional)
        purpose = ""
        policy_text = ""
        try:
            if self.manager.state.constitution:
                purpose = self.manager.state.constitution.purpose
                pol = getattr(self.manager.state.constitution, "creator_score_policy", None)
                if pol and getattr(pol, "enabled", False):
                    policy_text = (
                        "è©•ä¾¡ã¯Creatorã‚¹ã‚³ã‚¢(0-100)ã‚’æœ€é‡è¦KPIã¨ã™ã‚‹ã€‚"
                        f"å„ªå…ˆã¯ã€Œ{pol.priority}ã€ã€‚"
                        "å„è»¸ã¯ é¢ç™½ã•/ã‚³ã‚¹ãƒˆåŠ¹ç‡/ç¾å®Ÿæ€§/é€²åŒ–æ€§ï¼ˆå„0-25ï¼‰ã€‚"
                    )
        except Exception:
            pass

        latest_review = ""
        try:
            r = self.manager.creator_review_store.latest()
            if r:
                latest_review = f"ç›´è¿‘ãƒ¬ãƒ“ãƒ¥ãƒ¼: {r.score_total_100}/100 ã‚³ãƒ¡ãƒ³ãƒˆ: {r.comment}"
        except Exception:
            pass

        # Gather task history for context
        history_parts: list[str] = []
        pending_ids: list[str] = []
        try:
            completed = self.manager.task_queue.list_by_status("completed")
            completed.sort(key=lambda t: t.updated_at, reverse=True)
            for t in completed[:5]:
                result_short = (t.result or "")[:100]
                history_parts.append(f"  å®Œäº†: {t.description} â†’ {result_short}")
        except Exception:
            pass
        try:
            failed = self.manager.task_queue.list_by_status("failed")
            failed.sort(key=lambda t: t.updated_at, reverse=True)
            for t in failed[:3]:
                error_short = (t.error or "ä¸æ˜")[:100]
                history_parts.append(f"  å¤±æ•—: {t.description} â€” {error_short}")
        except Exception:
            pass
        try:
            pending = self.manager.task_queue.list_by_status("pending")
            for t in pending:
                pending_ids.append(f"  [{t.task_id}] {t.description}")
        except Exception:
            pass

        history_text = "\n".join(history_parts) if history_parts else "ãªã—"
        pending_text = "\n".join(pending_ids) if pending_ids else "ãªã—"

        prompt = (
            "ã‚ãªãŸã¯AIä¼šç¤¾ã®ç¤¾é•·AIã§ã™ã€‚\n"
            f"ç›®çš„: {purpose}\n"
            f"{policy_text}\n"
            f"{latest_review}\n"
            f"ãƒ“ã‚¸ãƒ§ãƒ³:\n{vision_text}\n\n"
            f"æœ€è¿‘ã®ã‚¿ã‚¹ã‚¯å±¥æ­´:\n{history_text}\n\n"
            f"æ—¢å­˜ã®pendingã‚¿ã‚¹ã‚¯:\n{pending_text}\n\n"
            "ç¾åœ¨pendingã®ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n"
            "ä¼šç¤¾ã®ãƒ“ã‚¸ãƒ§ãƒ³ã¨è©•ä¾¡æ–¹é‡ã«åŸºã¥ã„ã¦ã€æ¬¡ã«å–ã‚Šçµ„ã‚€ã¹ãæ–½ç­–ï¼ˆã‚¿ã‚¹ã‚¯ï¼‰ã‚’1ã€œ3å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚\n"
            "å„æ–½ç­–ã¯1è¡Œã§ç°¡æ½”ã«ã€‚å¯èƒ½ãªã‚‰ã€Œæœ€åˆã®ä¸€æ‰‹ã€ã¨ã€Œæƒ³å®šã‚¹ã‚³ã‚¢(é¢ç™½ã•/ã‚³ã‚¹ãƒˆåŠ¹ç‡/ç¾å®Ÿæ€§/é€²åŒ–æ€§)ã€ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚\n"
            "æ—¢å­˜ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã™ã‚‹å ´åˆã¯ depends_on:task_id1,task_id2 ã‚’æœ«å°¾ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚\n"
            "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:\n"
            "- æ–½ç­–1ã®èª¬æ˜ | æœ€åˆã®ä¸€æ‰‹: ... | æƒ³å®š: é¢ç™½ã•a/25 ã‚³ã‚¹ãƒˆåŠ¹ç‡b/25 ç¾å®Ÿæ€§c/25 é€²åŒ–æ€§d/25\n"
            "- æ–½ç­–2ã®èª¬æ˜ | depends_on:task_id1,task_id2\n"
        )

        messages = [
            {"role": "system", "content": "ã‚¿ã‚¹ã‚¯ææ¡ˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"},
            {"role": "user", "content": prompt},
        ]

        try:
            result = self.manager.llm_client.chat(messages)
        except Exception:
            logger.exception("LLM call failed during task proposal")
            return []

        if isinstance(result, LLMError):
            logger.error("LLM error during task proposal: %s", result.message)
            return []

        # Record LLM cost
        try:
            self.manager.record_llm_call(
                provider="openrouter",
                model=result.model,
                input_tokens=result.input_tokens,
                output_tokens=result.output_tokens,
                task_id="propose-tasks",
            )
        except Exception:
            logger.warning("Failed to record LLM call", exc_info=True)

        # Parse response to extract task descriptions
        tasks: list[TaskEntry] = []
        for line in result.content.splitlines():
            line = line.strip()
            # Match lines starting with "- " or "* " or numbered "1. "
            match = re.match(r"^[-*]\s+(.+)$|^\d+\.\s+(.+)$", line)
            if match:
                desc = (match.group(1) or match.group(2)).strip()
                # If the model included metadata (e.g. " | æœ€åˆã®ä¸€æ‰‹: ..."), keep only the core description.
                desc = desc.split("|", 1)[0].strip()
                if not desc:
                    continue

                # Parse depends_on:id1,id2 from the full line
                deps: list[str] = []
                dep_match = re.search(r"depends_on:\s*([\w,]+)", line)
                if dep_match:
                    deps = [d.strip() for d in dep_match.group(1).split(",") if d.strip()]

                try:
                    if deps:
                        entry = self.manager.task_queue.add_with_deps(desc, depends_on=deps)
                    else:
                        entry = self.manager.task_queue.add(desc)
                    tasks.append(entry)
                except Exception:
                    logger.warning("Failed to add proposed task: %s", desc, exc_info=True)

        logger.info("Proposed %d new tasks", len(tasks))
        return tasks

    def _execute_task(self, task: TaskEntry) -> None:
        """ã‚¿ã‚¹ã‚¯ã‚’LLMã«æ¸¡ã—ã¦å®Ÿè¡Œã™ã‚‹."""
        if self.manager.llm_client is None:
            logger.warning("LLM client not configured, cannot execute task")
            return

        # Update status to running
        try:
            self.manager.task_queue.update_status(task.task_id, "running")
        except Exception:
            logger.exception("Failed to update task status to running")
            return

        work_dir = self.manager.base_dir / "companies" / self.manager.company_id

        messages: list[dict[str, str]] = [
            {
                "role": "system",
                "content": (
                    "ã‚ãªãŸã¯AIä¼šç¤¾ã®ç¤¾é•·AIã§ã™ã€‚ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
                    "ã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰ãŒå¿…è¦ãªå ´åˆã¯<shell>ã‚³ãƒãƒ³ãƒ‰</shell>ã§æŒ‡ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
                    "Creatorã«ç›¸è«‡ãŒå¿…è¦ãªå ´åˆã¯<consult>ç›¸è«‡å†…å®¹</consult>ã§é€ã£ã¦ãã ã•ã„ã€‚\n"
                    "å®Œäº†ã—ãŸã‚‰<done>çµæœã®è¦ç´„</done>ã§å ±å‘Šã—ã¦ãã ã•ã„ã€‚"
                ),
            },
            {"role": "user", "content": f"ã‚¿ã‚¹ã‚¯: {task.description}"},
        ]

        try:
            had_shell = False
            for _turn in range(MAX_TASK_TURNS):
                # Budget check each turn
                if self.manager.check_budget():
                    self.manager.task_queue.update_status(
                        task.task_id, "failed", error="äºˆç®—è¶…é"
                    )
                    self._report(f"ã‚¿ã‚¹ã‚¯ä¸­æ–­(äºˆç®—è¶…é): {task.description}")
                    return

                result = self.manager.llm_client.chat(messages)

                if isinstance(result, LLMError):
                    self.manager.task_queue.update_status(
                        task.task_id, "failed", error=result.message
                    )
                    self._report(f"ã‚¿ã‚¹ã‚¯å¤±æ•—(LLMã‚¨ãƒ©ãƒ¼): {task.description}")
                    return

                # Record cost
                try:
                    self.manager.record_llm_call(
                        provider="openrouter",
                        model=result.model,
                        input_tokens=result.input_tokens,
                        output_tokens=result.output_tokens,
                        task_id=task.task_id,
                    )
                except Exception:
                    logger.warning("Failed to record LLM call", exc_info=True)

                messages.append({"role": "assistant", "content": result.content})
                actions = parse_response(result.content)

                done_result = None
                needs_followup = False

                for action in actions:
                    if action.action_type == "done":
                        done_result = action.content
                    elif action.action_type == "reply":
                        self._report(action.content)
                    elif action.action_type == "consult":
                        try:
                            entry = self.manager.consultation_store.add(
                                action.content.strip(),
                                related_task_id=task.task_id,
                            )
                            self._report(
                                f"ğŸ¤ ç›¸è«‡ [consult_id: {entry.consultation_id}]\n\n{action.content.strip()}"
                            )
                        except Exception:
                            self._report(f"ğŸ¤ ç›¸è«‡\n\n{action.content.strip()}")
                        self.manager.task_queue.update_status(
                            task.task_id, "failed", error="ç›¸è«‡å¾…ã¡"
                        )
                        return
                    elif action.action_type == "shell_command":
                        had_shell = True
                        shell_result = execute_shell(command=action.content, cwd=work_dir)
                        result_text = (
                            f"ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œçµæœ (return_code={shell_result.return_code}):\n"
                        )
                        if shell_result.stdout:
                            result_text += f"stdout:\n{shell_result.stdout}\n"
                        if shell_result.stderr:
                            result_text += f"stderr:\n{shell_result.stderr}\n"
                        messages.append({"role": "user", "content": result_text})
                        needs_followup = True
                        break
                    elif action.action_type == "delegate":
                        content = action.content.strip()
                        role, _, desc = content.partition(":")
                        role = role.strip() or "worker"
                        desc = desc.strip() or content
                        try:
                            sub_result = self.manager.sub_agent_runner.spawn(
                                name=role,
                                role=role,
                                task_description=desc,
                            )
                            result_text = f"ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµæœ (role={role}):\n{sub_result}"
                        except Exception as exc:
                            logger.warning("Sub-agent spawn failed: %s", exc, exc_info=True)
                            result_text = f"ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ (role={role}): {exc}"
                        messages.append({"role": "user", "content": result_text})
                        needs_followup = True
                        break

                if done_result is not None:
                    # Quality gate: verify output if shell commands were used
                    q_score, q_notes = None, None
                    enable_quality_gate = os.environ.get("TASK_QUALITY_GATE", "0") == "1"
                    if had_shell and enable_quality_gate:
                        try:
                            q_score, q_notes = self._verify_task_output(task, messages)
                        except Exception:
                            logger.warning("Quality verification failed", exc_info=True)

                    if q_score is not None and q_score < 0.5:
                        self.manager.task_queue.update_status(
                            task.task_id, "failed",
                            error=f"å“è³ªä¸è¶³ (score={q_score:.2f}): {q_notes}",
                            quality_score=q_score,
                            quality_notes=q_notes,
                        )
                        self._report(
                            f"ã‚¿ã‚¹ã‚¯å“è³ªä¸è¶³: {task.description}\n"
                            f"ã‚¹ã‚³ã‚¢: {q_score:.2f} â€” {q_notes}"
                        )
                    else:
                        self.manager.task_queue.update_status(
                            task.task_id, "completed", result=done_result,
                            quality_score=q_score,
                            quality_notes=q_notes,
                        )
                        self._report(f"ã‚¿ã‚¹ã‚¯å®Œäº†: {task.description}\nçµæœ: {done_result}")
                    return

                if not needs_followup:
                    # No shell and no done â€” treat as completed
                    self.manager.task_queue.update_status(
                        task.task_id, "completed", result=result.content
                    )
                    self._report(f"ã‚¿ã‚¹ã‚¯å®Œäº†: {task.description}")
                    return

            # Max turns reached
            self.manager.task_queue.update_status(
                task.task_id, "failed", error="æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°åˆ°é”"
            )
            self._report(f"ã‚¿ã‚¹ã‚¯ä¸­æ–­(æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°): {task.description}")

        except Exception as exc:
            logger.exception("Error executing task %s", task.task_id)
            try:
                self.manager.task_queue.update_status(
                    task.task_id, "failed", error=str(exc)
                )
            except Exception:
                logger.warning("Failed to update task status to failed", exc_info=True)
            self._report(f"ã‚¿ã‚¹ã‚¯å¤±æ•—(ã‚¨ãƒ©ãƒ¼): {task.description}")

    def _get_wip_limit(self) -> int:
        """WIPåˆ¶é™ã‚’å–å¾—ã™ã‚‹."""
        try:
            constitution = self.manager.state.constitution
            if constitution and constitution.work_principles:
                return constitution.work_principles.wip_limit
        except Exception:
            pass
        return DEFAULT_WIP_LIMIT
    def _verify_task_output(
        self, task: TaskEntry, conversation: list[dict[str, str]]
    ) -> tuple[float, str]:
        """LLMã«ã‚¿ã‚¹ã‚¯æˆæœç‰©ã®å“è³ªã‚’è©•ä¾¡ã•ã›ã‚‹.

        Returns (score, notes). On LLM failure returns (1.0, "verification skipped").
        """
        if self.manager.llm_client is None:
            return 1.0, "verification skipped: no LLM client"

        # Build a compact summary of the conversation for review
        summary_parts: list[str] = []
        for msg in conversation:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if len(content) > 500:
                content = content[:500] + "â€¦"
            summary_parts.append(f"[{role}] {content}")
        summary = "\n".join(summary_parts[-10:])  # last 10 messages

        review_prompt = (
            "ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ­ã‚°ã‚’ç¢ºèªã—ã€å“è³ªã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚\n"
            f"ã‚¿ã‚¹ã‚¯: {task.description}\n\n"
            f"å®Ÿè¡Œãƒ­ã‚°:\n{summary}\n\n"
            "ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„:\n"
            "score: 0.0ã€œ1.0ã®æ•°å€¤ï¼ˆ1.0ãŒæœ€é«˜å“è³ªï¼‰\n"
            "notes: è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ1è¡Œï¼‰\n"
        )

        messages = [
            {"role": "system", "content": "ã‚¿ã‚¹ã‚¯å“è³ªè©•ä¾¡ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€‚ç°¡æ½”ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": review_prompt},
        ]

        try:
            result = self.manager.llm_client.chat(messages)
        except Exception:
            logger.warning("Quality verification LLM call failed", exc_info=True)
            return 1.0, "verification skipped: LLM call exception"

        if isinstance(result, LLMError):
            logger.warning("Quality verification LLM error: %s", result.message)
            return 1.0, "verification skipped: LLM error"

        # Record cost
        try:
            self.manager.record_llm_call(
                provider="openrouter",
                model=result.model,
                input_tokens=result.input_tokens,
                output_tokens=result.output_tokens,
                task_id=task.task_id,
            )
        except Exception:
            logger.warning("Failed to record quality verification LLM call", exc_info=True)

        # Parse score and notes from response
        import re as _re
        score = 1.0
        notes = result.content.strip()

        score_match = _re.search(r"score:\s*([\d.]+)", result.content, _re.IGNORECASE)
        if score_match:
            try:
                parsed = float(score_match.group(1))
                if 0.0 <= parsed <= 1.0:
                    score = parsed
            except ValueError:
                pass

        notes_match = _re.search(r"notes:\s*(.+)", result.content, _re.IGNORECASE)
        if notes_match:
            notes = notes_match.group(1).strip()

        return score, notes

    def _report(self, message: str) -> None:
        """Slackã«çµæœã‚’å ±å‘Šã™ã‚‹."""
        try:
            self.manager._slack_send(message)
        except Exception:
            logger.warning("Failed to send report: %s", message, exc_info=True)
